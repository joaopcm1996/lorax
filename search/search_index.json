{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LoRAX","text":"<p> Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs </p> <p> </p>"},{"location":"#what-is-lorax","title":"\ud83d\udcd6 What is LoRAX?","text":"<p>LoRAX (LoRA eXchange) is a framework that allows users to serve thousands of fine-tuned models on a single GPU, dramatically reducing the cost of serving without compromising on throughput or latency.</p>"},{"location":"#features","title":"\ud83c\udf33 Features","text":"<ul> <li>\ud83d\ude85 Dynamic Adapter Loading: include any fine-tuned LoRA adapter in your request, it will be loaded just-in-time without blocking concurrent requests.</li> <li>\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f Heterogeneous Continuous Batching: packs requests for different adapters together into the same batch, keeping latency and throughput nearly constant with the number of concurrent adapters.</li> <li>\ud83e\uddc1 Adapter Exchange Scheduling: asynchronously prefetches and offloads adapters between GPU and CPU memory, schedules request batching to optimize the aggregate throughput of the system.</li> <li>\ud83d\udc6c Optimized Inference:  high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels (flash-attention, paged attention, SGMV), quantization, token streaming.</li> <li>\ud83d\udea2  Ready for Production prebuilt Docker images, Helm charts for Kubernetes, Prometheus metrics, and distributed tracing with Open Telemetry.</li> <li>\ud83e\udd2f Free for Commercial Use: Apache 2.0 License. Enough said \ud83d\ude0e.</li> </ul>"},{"location":"#models","title":"\ud83c\udfe0 Models","text":"<p>Serving a fine-tuned model with LoRAX consists of two components:</p> <ul> <li>Base Model: pretrained large model shared across all adapters.</li> <li>Adapter: task-specific adapter weights dynamically loaded per request.</li> </ul> <p>LoRAX supports a number of Large Language Models as the base model including Llama (including CodeLlama), Mistral (including Zephyr), and Qwen. See Supported Architectures for a complete list of supported base models.</p> <p>Base models can be loaded in fp16 or quantized with <code>bitsandbytes</code>, GPT-Q, or AWQ.</p> <p>Supported adapters include LoRA adapters trained using the PEFT and Ludwig libraries. Any of the linear layers in the model can be adapted via LoRA and loaded in LoRAX.</p>"},{"location":"#getting-started","title":"\ud83c\udfc3\u200d\u2642\ufe0f Getting Started","text":"<p>We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.</p>"},{"location":"#launch-lorax-server","title":"Launch LoRAX Server","text":"<pre><code>model=mistralai/Mistral-7B-Instruct-v0.1\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \\\n    ghcr.io/predibase/lorax:latest --model-id $model\n</code></pre> <p>For a full tutorial including token streaming and the Python client, see Getting Started - Docker.</p>"},{"location":"#prompt-via-rest-api","title":"Prompt via REST API","text":"<p>Prompt base LLM:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>Prompt a LoRA adapter:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>See Reference - REST API for full details.</p>"},{"location":"#prompt-via-python-client","title":"Prompt via Python Client","text":"<p>Install:</p> <pre><code>pip install lorax-client\n</code></pre> <p>Run:</p> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\n\n# Prompt the base LLM\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nprint(client.generate(prompt, max_new_tokens=64).generated_text)\n\n# Prompt a LoRA adapter\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\nprint(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)\n</code></pre> <p>See Reference - Python Client for full details.</p> <p>For other ways to run LoRAX, see Getting Started - Kubernetes, Getting Started - SkyPilot, and Getting Started - Local.</p>"},{"location":"#acknowledgements","title":"\ud83d\ude47 Acknowledgements","text":"<p>LoRAX is built on top of HuggingFace's text-generation-inference, forked from v0.9.4 (Apache 2.0).</p> <p>We'd also like to acknowledge Punica for their work on the SGMV kernel, which is used to speed up multi-adapter inference under heavy load.</p>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Our roadmap is tracked here.</p>"},{"location":"getting_started/docker/","title":"Docker","text":"<p>We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.</p>"},{"location":"getting_started/docker/#run-container-with-base-llm","title":"Run container with base LLM","text":"<p>In this example, we'll use Mistral-7B-Instruct as the base model, but you can use any supported model from HuggingFace.</p> <pre><code>model=mistralai/Mistral-7B-Instruct-v0.1\nvolume=$PWD/data  # share a volume with the container as a weight cache\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \\\n    ghcr.io/predibase/lorax:latest --model-id $model\n</code></pre> <p>Note</p> <p>To use GPUs, you need to install the NVIDIA Container Toolkit. We also recommend using NVIDIA drivers with CUDA version 11.8 or higher.</p> <p>See the references docs for the Launcher to view all available options, or run the following from within your container:</p> <pre><code>lorax-launcher --help\n</code></pre>"},{"location":"getting_started/docker/#prompt-the-base-llm","title":"Prompt the base LLM","text":"RESTREST (Streaming)PythonPython (Streaming) <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate_stream \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n\nprint(client.generate(prompt, max_new_tokens=64).generated_text)\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n\ntext = \"\"\nfor response in client.generate_stream(prompt, max_new_tokens=64):\n    if not response.token.special:\n        text += response.token.text\nprint(text)\n</code></pre>"},{"location":"getting_started/docker/#prompt-a-lora-adapter","title":"Prompt a LoRA adapter","text":"RESTREST (Streaming)PythonPython (Streaming) <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate_stream \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n\nprint(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n\ntext = \"\"\nfor response in client.generate_stream(prompt, max_new_tokens=64, adapter_id=adapter_id):\n    if not response.token.special:\n        text += response.token.text\nprint(text)\n</code></pre>"},{"location":"getting_started/kubernetes/","title":"Kubernetes (Helm)","text":"<p>LoRAX includes Helm charts that make it easy to start using LoRAX in production with high availability and load balancing on Kubernetes.</p> <p>To spin up a LoRAX deployment with Helm, you only need to be connected to a Kubernetes cluster through `kubectl``. We provide a default values.yaml file that can be used to deploy a Mistral 7B base model to your Kubernetes cluster:</p> <pre><code>helm install mistral-7b-release charts/lorax\n</code></pre> <p>The default values.yaml configuration deploys a single replica of the Mistral 7B model. You can tailor configuration parameters to deploy any Llama or Mistral model by creating a new values file from the template and updating variables. Once a new values file is created, you can run the following command to deploy your LLM with LoRAX:</p> <pre><code>helm install -f your-values-file.yaml your-model-release charts/lorax\n</code></pre> <p>To delete the resources:</p> <pre><code>helm uninstall your-model-release\n</code></pre>"},{"location":"getting_started/local/","title":"Local","text":"<p>Advanced users or contributors may opt to install LoRAX locally.</p> <p>First install Rust and create a Python virtual environment with at least Python 3.9, e.g. using <code>conda</code>:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\nconda create -n lorax python=3.9 \nconda activate lorax\n</code></pre> <p>You may also need to install Protoc.</p> <p>On Linux:</p> <pre><code>PROTOC_ZIP=protoc-21.12-linux-x86_64.zip\ncurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP\nsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc\nsudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'\nrm -f $PROTOC_ZIP\n</code></pre> <p>On MacOS, using Homebrew:</p> <pre><code>brew install protobuf\n</code></pre> <p>Then run:</p> <pre><code>BUILD_EXTENSIONS=True make install # Install repository and HF/transformer fork with CUDA kernels\nmake run-mistral-7b-instruct\n</code></pre> <p>Note: on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:</p> <pre><code>sudo apt-get install libssl-dev gcc -y\n</code></pre>"},{"location":"getting_started/local/#cuda-kernels","title":"CUDA Kernels","text":"<p>The custom CUDA kernels are only tested on NVIDIA A100s. If you have any installation or runtime issues, you can remove  the kernels by using the <code>DISABLE_CUSTOM_KERNELS=True</code> environment variable.</p> <p>Be aware that the official Docker image has them enabled by default.</p>"},{"location":"getting_started/local/#run-mistral","title":"Run Mistral","text":"<pre><code>make run-mistral-7b-instruct\n</code></pre>"},{"location":"getting_started/skypilot/","title":"SkyPilot","text":"<p>SkyPilot is a framework for running AI workloads in the cloud of your choice (AWS, Azure, GCP, etc.). It abstracts away the complexity of finding available GPU resources across clouds / zones, syncing data between storage systems, and managing the excution of distributed workloads.</p>"},{"location":"getting_started/skypilot/#setup","title":"Setup","text":"<p>First install SkyPilot and check that your cloud credentials are properly set:</p> <pre><code>pip install skypilot\nsky check\n</code></pre>"},{"location":"getting_started/skypilot/#launch-a-deployment","title":"Launch a deployment","text":"<p>Create a YAML configuration file called <code>lorax.yaml</code>:</p> <pre><code>resources:\n  cloud: aws\n  accelerators: A10G:1\n  memory: 32+\n  ports: \n    - 8080\n\nenvs:\n  MODEL_ID: mistralai/Mistral-7B-Instruct-v0.1\n\nrun: |\n  docker run --gpus all --shm-size 1g -p 8080:80 -v ~/data:/data \\\n    ghcr.io/predibase/lorax:latest \\\n    --model-id $MODEL_ID\n</code></pre> <p>In the above example, we're asking SkyPilot to provision an AWS instance with 1 Nvidia A10G GPU and at least 32GB of RAM. Once the node is provisioned, SkyPilot will launch the LoRAX server using our latest pre-built Docker image.</p> <p>Let's launch our LoRAX job:</p> <pre><code>sky launch -c lorax-cluster lorax.yaml\n</code></pre> <p>By default, this config will deploy Mistral-7B-Instruct, but this can be overridden by running <code>sky launch</code> with the argument <code>--env MODEL_ID=&lt;my_model&gt;</code>.</p> <p>Warn</p> <p>This config will launch the instance on a public IP. It's highly recommended to secure the instance within a private subnet. See the Advanced Configurations section of the SkyPilot docs for options to run within VPC and setup private IPs.</p>"},{"location":"getting_started/skypilot/#prompt-lorax","title":"Prompt LoRAX","text":"<p>In a separate window, obtain the IP address of the newly created instance:</p> <pre><code>sky status --ip lorax-cluster\n</code></pre> <p>Now we can prompt the LoRAX deployment as usual:</p> <pre><code>IP=$(sky status --ip lorax-cluster)\n\ncurl http://$IP:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre>"},{"location":"getting_started/skypilot/#stop-the-deployment","title":"Stop the deployment","text":"<p>Stopping the deployment will shut down the instance, but keep the storage volume:</p> <pre><code>sky stop lorax-cluster\n</code></pre> <p>Because we set <code>docker run ... -v ~/data:/data</code> in our config from before, this means any model weights or adapters we downloaded will be persisted the next time we run <code>sky launch</code>. The LoRAX Docker image will also be cached, meaning tags like <code>latest</code> won't be updated on restart unless you add <code>docker pull</code> to your <code>run</code> configuration.</p>"},{"location":"getting_started/skypilot/#delete-the-deployment","title":"Delete the deployment","text":"<p>To completely delete the deployment, including the storage volume:</p> <pre><code>sky down lorax-cluster\n</code></pre> <p>The next time you run <code>sky launch</code>, the deployment will be recreated from scratch.</p>"},{"location":"guides/cuda_graphs/","title":"CUDA Graph Compilation","text":"<p>LoRAX supports compiling the model into a static CUDA Graph to speedup inference by upwards of 2x. See Accelerating PyTorch with CUDA Graphs for more details on CUDA graphs and how they can reduce latency.</p>"},{"location":"guides/cuda_graphs/#usage","title":"Usage","text":"<p>To enable this (experimental) feature:</p> <pre><code>lorax-launcher ... --compile\n</code></pre>"},{"location":"guides/cuda_graphs/#when-should-i-use-this","title":"When should I use this?","text":"<p>CUDA graph compilation is a simple way to decrease latency for smaller LLMs (O(1b params)) that are compute bound rather than memory bound.</p> <p>There is a tradeoff to be aware of when using CUDA graphs, namely that it increases memory overhead by 3-10GB depending on model size. However, the observed decrease in latency can be as much as 50%, so if you don't need to run with very large batch sizes and are more latency constrained than throughput, this is a very compelling feature to enable.</p> <p>In practice, CUDA graphs are most useful in cases where there are excess GPU flops available, such as during decoding. As such, we do not use the compiled version of the model during prefill, only during the decoding steps. Which means in practice that the benefits of enabling compilation will be most pronounced when generating longer sequences (for which more time is spent during decoding).</p>"},{"location":"guides/cuda_graphs/#limitations","title":"Limitations","text":"<p>Current limitations:</p> <ul> <li>Batch size &lt; 256</li> <li>Context length (input + output) &lt; 8192</li> <li>LoRA rank &gt;= 8 and &lt;= 64</li> <li>Only one LoRA rank in the batch</li> <li>1 GPU (no sharding)</li> </ul> <p>If any of these conditions are not met, then LoRAX will fallback to using eager execution for the batch.</p>"},{"location":"guides/cuda_graphs/#benchmarks","title":"Benchmarks","text":"<p>gpt2-medium, 1x A100, time to generate 100 tokens:</p> <p>no adapter:</p> <ul> <li>baseline: 1.044 s</li> <li>cuda graph: 0.422 s</li> </ul> <p>1 adapter (rank 16):</p> <ul> <li>baseline: 1.503 s</li> <li>cuda graph: 0.583 s</li> </ul>"},{"location":"models/adapters/","title":"Adapters","text":"<p>LoRAX currently supports LoRA adapters, which can be trained using frameworks like PEFT and Ludwig.</p>"},{"location":"models/adapters/#target-modules","title":"Target Modules","text":"<p>Any combination of linear layers can be targeted in the adapters, which corresponds to the following target modules for each base model.</p>"},{"location":"models/adapters/#llama","title":"Llama","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/#mistral","title":"Mistral","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/#mixtral","title":"Mixtral","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/#qwen","title":"Qwen","text":"<ul> <li><code>c_attn</code></li> <li><code>c_proj</code></li> <li><code>w1</code></li> <li><code>w2</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/#phi","title":"Phi","text":"<ul> <li><code>Wqkv</code></li> <li><code>out_proj</code></li> <li><code>fc1</code></li> <li><code>fc2</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/#gpt2","title":"GPT2","text":"<ul> <li><code>c_attn</code></li> <li><code>c_proj</code></li> <li><code>c_fc</code></li> </ul>"},{"location":"models/adapters/#source","title":"Source","text":"<p>You can provide an adapter from the HuggingFace Hub, a local file path, or S3. </p> <p>Just make sure that the adapter was trained on the same base model used in the deployment. LoRAX only supports one base model at a time, but any number of adapters derived from it!</p>"},{"location":"models/adapters/#huggingface-hub","title":"Huggingface Hub","text":"<p>By default, LoRAX will load adapters from the Huggingface Hub.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"hub\",\n}\n</code></pre>"},{"location":"models/adapters/#local","title":"Local","text":"<p>When specifying an adapter in a local path, the <code>adapter_id</code> should correspond to the root directory of the adapter containing the following files:</p> <pre><code>root_adapter_path/\n    adapter_config.json\n    adapter_model.bin\n    adapter_model.safetensors\n</code></pre> <p>The weights must be in one of either a <code>adapter_model.bin</code> (pickle) or <code>adapter_model.safetensors</code> (safetensors) format. If both are provided, safestensors will be used.</p> <p>See the PEFT library for detailed examples showing how to save adapters in this format.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"/data/adapters/vineetsharma--qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"local\",\n}\n</code></pre>"},{"location":"models/adapters/#s3","title":"S3","text":"<p>Similar to a local path, an S3 path can be provided. Just make sure you have the appropriate environment variables <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> set so you can authenticate to AWS.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"s3://adapters_bucket/vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"s3\",\n}\n</code></pre>"},{"location":"models/base_models/","title":"Base Models","text":""},{"location":"models/base_models/#supported-architectures","title":"Supported Architectures","text":"<ul> <li>\ud83e\udd99 Llama<ul> <li>CodeLlama</li> </ul> </li> <li>\ud83c\udf2c\ufe0fMistral<ul> <li>Zephyr</li> </ul> </li> <li>\ud83d\udd04 Mixtral</li> <li>\ud83d\udd2e Qwen</li> <li>\ud83c\udfdb\ufe0f Phi</li> <li>\ud83e\udd16 GPT2</li> </ul> <p>Other architectures are supported on a best effort basis, but do not support dynamic adapter loading.</p>"},{"location":"models/base_models/#selecting-a-base-model","title":"Selecting a Base Model","text":"<p>Check the HuggingFace Hub to find supported base models.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 ...\n</code></pre>"},{"location":"models/base_models/#quantization","title":"Quantization","text":"<p>Base models can be loaded in fp16 (default) or with quantization using any of <code>bitsandbytes</code>, GPT-Q, or AWQ format. When using quantization, it is not necessary that the adapter was fine-tuned using the quantized version of the base model, but be aware that enabling quantization can have an effect on the response.</p>"},{"location":"models/base_models/#bitsandbytes","title":"bitsandbytes","text":"<p><code>bitsandbytes</code> quantization can be applied to any base model saved in fp16 or bf16 format. It performs quantization at runtime in a model and dataset agnostic manner. As such, it is more flexible but potentially less performant (both in terms of quality and latency) than other quantization options.</p> <p>There are three flavors of <code>bitsandbytes</code> quantization:</p> <ul> <li><code>bitsandbytes</code> (8-bit integer)</li> <li><code>bitsandbytes-fp4</code> (4-bit float)</li> <li><code>bitsandbytes-nf4</code> (4-bit normal float)</li> </ul> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize bitsandbytes-nf4 ...\n</code></pre>"},{"location":"models/base_models/#gpt-q","title":"GPT-Q","text":"<p>GPT-Q is a static quantization method, meaning that the quantization needs to be done outside of LoRAX and the weights persisted in order for it to be used with a base model. Thanks to the ExLlama-v2 CUDA kernels, GPT-Q offers very strong inference performance compared with <code>bitsandbytes</code>, but may not generalize as well to unseen tasks (as the quantization is done with respect to a particular dataset). </p> <p>Usage:</p> <pre><code>lorax-launcher --model-id TheBloke/Mistral-7B-v0.1-GPTQ --quantize gptq ...\n</code></pre>"},{"location":"models/base_models/#awq","title":"AWQ","text":"<p>AWQ is similar to GPT-Q in that the weights are quantized in advance of inference (statically). AWQ is generally faster than GPT-Q for inference, and achieves similarly high levels of quality.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id TheBloke/Mistral-7B-v0.1-AWQ --quantize awq ...\n</code></pre>"},{"location":"reference/launcher/","title":"LoRAX Launcher","text":"<pre><code>LoRAX Launcher\n\nUsage: lorax-launcher [OPTIONS]\n\nOptions:\n      --model-id &lt;MODEL_ID&gt;\n          The name of the model to load. Can be a MODEL_ID as listed on &lt;https://hf.co/models&gt; like `gpt2` or `mistralai/Mistral-7B-Instruct-v0.1`. Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of transformers\n\n          [env: MODEL_ID=]\n          [default: mistralai/Mistral-7B-Instruct-v0.1]\n\n      --adapter-id &lt;ADAPTER_ID&gt;\n          The name of the adapter to load. Can be a MODEL_ID as listed on &lt;https://hf.co/models&gt; or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of transformers. Should be compatible with the model specified in `model_id`\n\n          [env: ADAPTER_ID=]\n          [default: ]\n\n      --source &lt;SOURCE&gt;\n          The source of the model to load. Can be `hub` or `s3`. `hub` will load the model from the huggingface hub. `s3` will load the model from the predibase S3 bucket\n\n          [env: SOURCE=]\n          [default: hub]\n\n      --adapter-source &lt;ADAPTER_SOURCE&gt;\n          The source of the model to load. Can be `hub` or `s3` or `pbase` `hub` will load the model from the huggingface hub. `s3` will load the model from the predibase S3 bucket. `pbase` will load an s3 model but resolve the metadata from a predibase server\n\n          [env: ADAPTER_SOURCE=]\n          [default: hub]\n\n      --revision &lt;REVISION&gt;\n          The actual revision of the model if you're referring to a model on the hub. You can use a specific commit id or a branch like `refs/pr/2`\n\n          [env: REVISION=]\n\n      --validation-workers &lt;VALIDATION_WORKERS&gt;\n          The number of tokenizer workers used for payload validation and truncation inside the router\n\n          [env: VALIDATION_WORKERS=]\n          [default: 2]\n\n      --sharded &lt;SHARDED&gt;\n          Whether to shard the model across multiple GPUs By default LoRAX will use all available GPUs to run the model. Setting it to `false` deactivates `num_shard`\n\n          [env: SHARDED=]\n          [possible values: true, false]\n\n      --num-shard &lt;NUM_SHARD&gt;\n          The number of shards to use if you don't want to use all GPUs on a given machine. You can use `CUDA_VISIBLE_DEVICES=0,1 lorax-launcher... --num_shard 2` and `CUDA_VISIBLE_DEVICES=2,3 lorax-launcher... --num_shard 2` to launch 2 copies with 2 shard each on a given machine with 4 GPUs for instance\n\n          [env: NUM_SHARD=]\n\n      --quantize &lt;QUANTIZE&gt;\n          Whether you want the model to be quantized. This will use `bitsandbytes` for quantization on the fly, or `gptq`\n\n          [env: QUANTIZE=]\n          [possible values: bitsandbytes, bitsandbytes-nf4, bitsandbytes-fp4, gptq, awq]\n\n      --compile\n          Whether you want to compile the model into a CUDA graph. This will speed up decoding but increase GPU memory usage\n\n          [env: COMPILE=]\n\n      --dtype &lt;DTYPE&gt;\n          The dtype to be forced upon the model. This option cannot be used with `--quantize`\n\n          [env: DTYPE=]\n          [possible values: float16, bfloat16]\n\n      --trust-remote-code\n          Whether you want to execute hub modelling code. Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision\n\n          [env: TRUST_REMOTE_CODE=]\n\n      --max-concurrent-requests &lt;MAX_CONCURRENT_REQUESTS&gt;\n          The maximum amount of concurrent requests for this particular deployment. Having a low limit will refuse clients requests instead of having them wait for too long and is usually good to handle backpressure correctly\n\n          [env: MAX_CONCURRENT_REQUESTS=]\n          [default: 128]\n\n      --max-best-of &lt;MAX_BEST_OF&gt;\n          This is the maximum allowed value for clients to set `best_of`. Best of makes `n` generations at the same time, and return the best in terms of overall log probability over the entire generated sequence\n\n          [env: MAX_BEST_OF=]\n          [default: 2]\n\n      --max-stop-sequences &lt;MAX_STOP_SEQUENCES&gt;\n          This is the maximum allowed value for clients to set `stop_sequences`. Stop sequences are used to allow the model to stop on more than just the EOS token, and enable more complex \"prompting\" where users can preprompt the model in a specific way and define their \"own\" stop token aligned with their prompt\n\n          [env: MAX_STOP_SEQUENCES=]\n          [default: 4]\n\n      --max-input-length &lt;MAX_INPUT_LENGTH&gt;\n          This is the maximum allowed input length (expressed in number of tokens) for users. The larger this value, the longer prompt users can send which can impact the overall memory required to handle the load. Please note that some models have a finite range of sequence they can handle\n\n          [env: MAX_INPUT_LENGTH=]\n          [default: 1024]\n\n      --max-total-tokens &lt;MAX_TOTAL_TOKENS&gt;\n          This is the most important value to set as it defines the \"memory budget\" of running clients requests. Clients will send input sequences and ask to generate `max_new_tokens` on top. with a value of `1512` users can send either a prompt of `1000` and ask for `512` new tokens, or send a prompt of `1` and ask for `1511` max_new_tokens. The larger this value, the larger amount each request will be in your RAM and the less effective batching can be\n\n          [env: MAX_TOTAL_TOKENS=]\n          [default: 2048]\n\n      --waiting-served-ratio &lt;WAITING_SERVED_RATIO&gt;\n          This represents the ratio of waiting queries vs running queries where you want to start considering pausing the running queries to include the waiting ones into the same batch. `waiting_served_ratio=1.2` Means when 12 queries are waiting and there's only 10 queries left in the current batch we check if we can fit those 12 waiting queries into the batching strategy, and if yes, then batching happens delaying the 10 running queries by a `prefill` run.\n\n          This setting is only applied if there is room in the batch as defined by `max_batch_total_tokens`.\n\n          [env: WAITING_SERVED_RATIO=]\n          [default: 1.2]\n\n      --max-batch-prefill-tokens &lt;MAX_BATCH_PREFILL_TOKENS&gt;\n          Limits the number of tokens for the prefill operation. Since this operation take the most memory and is compute bound, it is interesting to limit the number of requests that can be sent\n\n          [env: MAX_BATCH_PREFILL_TOKENS=]\n          [default: 4096]\n\n      --max-batch-total-tokens &lt;MAX_BATCH_TOTAL_TOKENS&gt;\n          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.\n\n          This represents the total amount of potential tokens within a batch. When using padding (not recommended) this would be equivalent of `batch_size` * `max_total_tokens`.\n\n          However in the non-padded (flash attention) version this can be much finer.\n\n          For `max_batch_total_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.\n\n          Overall this number should be the largest possible amount that fits the remaining memory (after the model is loaded). Since the actual memory overhead depends on other parameters like if you're using quantization, flash attention or the model implementation, LoRAX cannot infer this number automatically.\n\n          [env: MAX_BATCH_TOTAL_TOKENS=]\n\n      --max-waiting-tokens &lt;MAX_WAITING_TOKENS&gt;\n          This setting defines how many tokens can be passed before forcing the waiting queries to be put on the batch (if the size of the batch allows for it). New queries require 1 `prefill` forward, which is different from `decode` and therefore you need to pause the running batch in order to run `prefill` to create the correct values for the waiting queries to be able to join the batch.\n\n          With a value too small, queries will always \"steal\" the compute to run `prefill` and running queries will be delayed by a lot.\n\n          With a value too big, waiting queries could wait for a very long time before being allowed a slot in the running batch. If your server is busy that means that requests that could run in ~2s on an empty server could end up running in ~20s because the query had to wait for 18s.\n\n          This number is expressed in number of tokens to make it a bit more \"model\" agnostic, but what should really matter is the overall latency for end users.\n\n          [env: MAX_WAITING_TOKENS=]\n          [default: 20]\n\n      --max-active-adapters &lt;MAX_ACTIVE_ADAPTERS&gt;\n          Maximum number of adapters that can be placed on the GPU and accept requests at a time\n\n          [env: MAX_ACTIVE_ADAPTERS=]\n          [default: 128]\n\n      --adapter-cycle-time-s &lt;ADAPTER_CYCLE_TIME_S&gt;\n          The time in seconds between adapter exchanges\n\n          [env: ADAPTER_CYCLE_TIME_S=]\n          [default: 2]\n\n      --hostname &lt;HOSTNAME&gt;\n          The IP address to listen on\n\n          [env: HOSTNAME=]\n          [default: 0.0.0.0]\n\n  -p, --port &lt;PORT&gt;\n          The port to listen on\n\n          [env: PORT=]\n          [default: 3000]\n\n      --shard-uds-path &lt;SHARD_UDS_PATH&gt;\n          The name of the socket for gRPC communication between the webserver and the shards\n\n          [env: SHARD_UDS_PATH=]\n          [default: /tmp/lorax-server]\n\n      --master-addr &lt;MASTER_ADDR&gt;\n          The address the master shard will listen on. (setting used by torch distributed)\n\n          [env: MASTER_ADDR=]\n          [default: localhost]\n\n      --master-port &lt;MASTER_PORT&gt;\n          The address the master port will listen on. (setting used by torch distributed)\n\n          [env: MASTER_PORT=]\n          [default: 29500]\n\n      --huggingface-hub-cache &lt;HUGGINGFACE_HUB_CACHE&gt;\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk for instance\n\n          [env: HUGGINGFACE_HUB_CACHE=]\n\n      --weights-cache-override &lt;WEIGHTS_CACHE_OVERRIDE&gt;\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk for instance\n\n          [env: WEIGHTS_CACHE_OVERRIDE=]\n\n      --disable-custom-kernels\n          For some models (like llama), LoRAX implemented custom cuda kernels to speed up inference. Those kernels were only tested on A100. Use this flag to disable them if you're running on different hardware and encounter issues\n\n          [env: DISABLE_CUSTOM_KERNELS=]\n\n      --cuda-memory-fraction &lt;CUDA_MEMORY_FRACTION&gt;\n          Limit the CUDA available memory. The allowed value equals the total visible memory multiplied by cuda-memory-fraction\n\n          [env: CUDA_MEMORY_FRACTION=]\n          [default: 1.0]\n\n      --json-output\n          Outputs the logs in JSON format (useful for telemetry)\n\n          [env: JSON_OUTPUT=]\n\n      --otlp-endpoint &lt;OTLP_ENDPOINT&gt;\n          [env: OTLP_ENDPOINT=]\n\n      --cors-allow-origin &lt;CORS_ALLOW_ORIGIN&gt;\n          [env: CORS_ALLOW_ORIGIN=]\n\n      --watermark-gamma &lt;WATERMARK_GAMMA&gt;\n          [env: WATERMARK_GAMMA=]\n\n      --watermark-delta &lt;WATERMARK_DELTA&gt;\n          [env: WATERMARK_DELTA=]\n\n      --ngrok\n          Enable ngrok tunneling\n\n          [env: NGROK=]\n\n      --ngrok-authtoken &lt;NGROK_AUTHTOKEN&gt;\n          ngrok authentication token\n\n          [env: NGROK_AUTHTOKEN=]\n\n      --ngrok-edge &lt;NGROK_EDGE&gt;\n          ngrok edge\n\n          [env: NGROK_EDGE=]\n\n  -e, --env\n          Display a lot of information about your runtime environment\n\n      --download-only\n          Download model weights only\n\n          [env: DOWNLOAD_ONLY=]\n\n  -h, --help\n          Print help (see a summary with '-h')\n\n  -V, --version\n          Print version\n</code></pre>"},{"location":"reference/python_client/","title":"Python Client","text":"<p>LoRAX Python client provides a convenient way of interfacing with a <code>lorax</code> instance running in your environment.</p>"},{"location":"reference/python_client/#install","title":"Install","text":"<pre><code>pip install lorax-client\n</code></pre>"},{"location":"reference/python_client/#usage","title":"Usage","text":"<pre><code>from lorax import Client\n\nendpoint_url = \"http://127.0.0.1:8080\"\n\nclient = Client(endpoint_url)\ntext = client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\").generated_text\nprint(text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nfor response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n</code></pre> <p>or with the asynchronous client:</p> <pre><code>from lorax import AsyncClient\n\nendpoint_url = \"http://127.0.0.1:8080\"\n\nclient = AsyncClient(endpoint_url)\nresponse = await client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\")\nprint(response.generated_text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nasync for response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n</code></pre>"},{"location":"reference/python_client/#predibase-inference-endpoints","title":"Predibase Inference Endpoints","text":"<p>The LoRAX client can also be used to connect to Predibase managed LoRAX endpoints (including Predibase's serverless endpoints).</p> <p>You need only make the following changes to the above examples:</p> <ol> <li>Change the <code>endpoint_url</code> to match the endpoint of your Predibase LLM of choice.</li> <li>Provide your Predibase API token in the <code>headers</code> provided to the client.</li> </ol> <p>Example:</p> <pre><code>from lorax import Client\n\n# You can get your Predibase API token by going to Settings &gt; My Profile &gt; Generate API Token\n# You can get your Predibase Tenant short code by going to Settings &gt; My Profile &gt; Overview &gt; Tenant ID\nendpoint_url = f\"https://serving.app.predibase.com/{predibase_tenant_short_code}/deployments/v2/llms/{llm_deployment_name}\"\nheaders = {\n    \"Authorization\": f\"Bearer {api_token}\"\n}\n\nclient = Client(endpoint_url, headers=headers)\n\n# same as above from here ...\nresponse = client.generate(\"Why is the sky blue?\", adapter_id=f\"{model_repo}/{model_version}\")\n</code></pre> <p>Note that by default Predibase will use its internal model repos as the default <code>adapter_source</code>. To use an adapter from Huggingface:</p> <pre><code>response = client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\", adapter_source=\"hub\")\n</code></pre>"},{"location":"reference/python_client/#types","title":"Types","text":"<pre><code># Request Parameters\nclass Parameters:\n    # The ID of the adapter to use\n    adapter_id: Optional[str]\n    # The source of the adapter to use\n    adapter_source: Optional[str]\n    # Activate logits sampling\n    do_sample: bool\n    # Maximum number of generated tokens\n    max_new_tokens: int\n    # The parameter for repetition penalty. 1.0 means no penalty.\n    # See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n    repetition_penalty: Optional[float]\n    # Whether to prepend the prompt to the generated text\n    return_full_text: bool\n    # Stop generating tokens if a member of `stop_sequences` is generated\n    stop: List[str]\n    # Random sampling seed\n    seed: Optional[int]\n    # The value used to module the logits distribution.\n    temperature: Optional[float]\n    # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n    top_k: Optional[int]\n    # If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n    # higher are kept for generation.\n    top_p: Optional[float]\n    # truncate inputs tokens to the given size\n    truncate: Optional[int]\n    # Typical Decoding mass\n    # See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information\n    typical_p: Optional[float]\n    # Generate best_of sequences and return the one if the highest token logprobs\n    best_of: Optional[int]\n    # Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\n    watermark: bool\n    # Get decoder input token logprobs and ids\n    decoder_input_details: bool\n\n# Decoder input tokens\nclass InputToken:\n    # Token ID from the model tokenizer\n    id: int\n    # Token text\n    text: str\n    # Logprob\n    # Optional since the logprob of the first token cannot be computed\n    logprob: Optional[float]\n\n\n# Generated tokens\nclass Token:\n    # Token ID from the model tokenizer\n    id: int\n    # Token text\n    text: str\n    # Logprob\n    logprob: float\n    # Is the token a special token\n    # Can be used to ignore tokens when concatenating\n    special: bool\n\n\n# Generation finish reason\nclass FinishReason(Enum):\n    # number of generated tokens == `max_new_tokens`\n    Length = \"length\"\n    # the model generated its end of sequence token\n    EndOfSequenceToken = \"eos_token\"\n    # the model generated a text included in `stop_sequences`\n    StopSequence = \"stop_sequence\"\n\n\n# Additional sequences when using the `best_of` parameter\nclass BestOfSequence:\n    # Generated text\n    generated_text: str\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n    # Decoder input tokens, empty if decoder_input_details is False\n    prefill: List[InputToken]\n    # Generated tokens\n    tokens: List[Token]\n\n\n# `generate` details\nclass Details:\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n    # Decoder input tokens, empty if decoder_input_details is False\n    prefill: List[InputToken]\n    # Generated tokens\n    tokens: List[Token]\n    # Additional sequences when using the `best_of` parameter\n    best_of_sequences: Optional[List[BestOfSequence]]\n\n\n# `generate` return value\nclass Response:\n    # Generated text\n    generated_text: str\n    # Generation details\n    details: Details\n\n\n# `generate_stream` details\nclass StreamDetails:\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n\n\n# `generate_stream` return value\nclass StreamResponse:\n    # Generated token\n    token: Token\n    # Complete generated text\n    # Only available when the generation is finished\n    generated_text: Optional[str]\n    # Generation details\n    # Only available when the generation is finished\n    details: Optional[StreamDetails]\n\n# Inference API currently deployed model\nclass DeployedModel:\n    model_id: str\n    sha: str\n</code></pre>"},{"location":"reference/rest_api/","title":"REST API","text":""}]}